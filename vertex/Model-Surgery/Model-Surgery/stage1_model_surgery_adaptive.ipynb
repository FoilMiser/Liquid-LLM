{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage-1 Model Surgery (Adaptive) \u2014 Liquid \u2192 Hybrid (Net2Net widen + new layers)\n",
    "\n",
    "This notebook:\n",
    "- Introspects your student checkpoint to infer d_model, n_layers, and vocab size.\n",
    "- Widens all tensors that depend on d_model by +10% (rounded to a multiple of 8).\n",
    "- Appends 2 classic (transformer-style) layers (tiny-out init) and 3 liquid layers (small-scale init).\n",
    "- Writes out: a new checkpoint, config_stage1.json, freeze_mask.json, and surgery_report.md.\n",
    "\n",
    "> Fill in the two placeholders for CHECKPOINT_PATH and OUTPUT_DIR below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: imports, config, and IO paths\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.init as nn_init\n",
    "\n",
    "if hasattr(torch, \"set_float32_matmul_precision\"):\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "SURGERY_DTYPE = torch.float32\n",
    "\n",
    "CHECKPOINT_PATH = \"/path/to/stage0_student.pt\"\n",
    "OUTPUT_DIR = \"./_stage1_out\"\n",
    "\n",
    "WIDTH_SCALE = 1.10\n",
    "ROUND_MULT = 8\n",
    "ADD_CLASSIC_LAY = 2\n",
    "ADD_LIQUID_LAY = 3\n",
    "WIDTH_TOLERANCE = 0.06\n",
    "DEFAULT_MAX_SEQ = 4096\n",
    "MAX_K_MATCH = 8\n",
    "\n",
    "CUSTOM_LAYER_REGEX = \"\"  # e.g., r\"^transformer\\.h\\.(\\d+)\\.(.+)$\"\n\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: checkpoint IO + layer schema discovery\n",
    "PREFIXES_TO_STRIP = (\n",
    "    \"module.\",\n",
    "    \"model.\",\n",
    "    \"student.\",\n",
    "    \"state_dict.\",\n",
    "    \"network.\",\n",
    ")\n",
    "\n",
    "\n",
    "def _is_placeholder(value: Optional[str]) -> bool:\n",
    "    if value is None:\n",
    "        return True\n",
    "    value = value.strip()\n",
    "    if not value:\n",
    "        return True\n",
    "    placeholder_tokens = {\"/path/to\", \"CHANGE_ME\", \"your/checkpoint\", \"gs://bucket\"}\n",
    "    return any(token in value for token in placeholder_tokens)\n",
    "\n",
    "\n",
    "def strip_known_prefixes(key: str) -> str:\n",
    "    new_key = key\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for prefix in PREFIXES_TO_STRIP:\n",
    "            if new_key.startswith(prefix):\n",
    "                new_key = new_key[len(prefix) :]\n",
    "                changed = True\n",
    "    return new_key\n",
    "\n",
    "\n",
    "def _safe_torch_load(path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Try to load with weights_only=True first (PyTorch >=2.4 behavior),\n",
    "    then fall back to weights_only=False if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "    except TypeError:\n",
    "        # older torch doesn't support weights_only\n",
    "        return torch.load(path, map_location=\"cpu\")\n",
    "    except Exception:\n",
    "        raise\n",
    "\n",
    "\n",
    "def _safe_load_any(path: str) -> Any:\n",
    "    \"\"\"Load torch or safetensors checkpoints safely.\"\"\"\n",
    "    if path.lower().endswith(\".safetensors\"):\n",
    "        # Prefer safetensors for .safetensors files when available.\n",
    "        try:\n",
    "            from safetensors.torch import load_file\n",
    "        except ImportError:\n",
    "            return _safe_torch_load(path)\n",
    "        return load_file(path, device=\"cpu\")\n",
    "    return _safe_torch_load(path)\n",
    "\n",
    "\n",
    "def _extract_state_dict_root(obj: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Given an arbitrary checkpoint object, return the most likely state_dict-ish mapping.\n",
    "    Handles common wrappers: 'state_dict', 'model', 'module', 'model_state_dict', 'ema_state_dict'.\n",
    "    If multiple candidates exist, prefer 'state_dict' > 'model' > 'module' > '*state_dict'.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (dict, OrderedDict)):\n",
    "        vals = list(obj.values())\n",
    "        if vals and sum(isinstance(v, torch.Tensor) for v in vals) / len(vals) > 0.6:\n",
    "            return obj\n",
    "        for key in [\"state_dict\", \"model\", \"module\", \"model_state_dict\", \"ema_state_dict\"]:\n",
    "            if key in obj and isinstance(obj[key], (dict, OrderedDict)):\n",
    "                return obj[key]\n",
    "        for v in obj.values():\n",
    "            if isinstance(v, (dict, OrderedDict)):\n",
    "                return v\n",
    "    raise TypeError(\"Checkpoint must be a mapping that contains a state_dict-like mapping.\")\n",
    "\n",
    "\n",
    "def _flatten_nested_state_dict(sd_like: Any, prefix: str = \"\") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Recursively flatten nested mappings into a single dict[str, Tensor].\n",
    "    Only include leaves that are torch.Tensors; ignore non-tensor leaves safely.\n",
    "    \"\"\"\n",
    "    flat = {}\n",
    "    stack = [(prefix, sd_like)]\n",
    "    while stack:\n",
    "        pfx, node = stack.pop()\n",
    "        if isinstance(node, (dict, OrderedDict)):\n",
    "            for k, v in node.items():\n",
    "                key = f\"{pfx}{k}\" if pfx == \"\" else f\"{pfx}.{k}\"\n",
    "                if isinstance(v, (dict, OrderedDict)):\n",
    "                    stack.append((key, v))\n",
    "                elif isinstance(v, torch.Tensor):\n",
    "                    flat[key] = v.detach().cpu()\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "    if not flat:\n",
    "        raise ValueError(\"No tensor leaves found in checkpoint. Is this a valid state_dict?\")\n",
    "    return flat\n",
    "\n",
    "\n",
    "def load_stage0_state_dict(path: str) -> Tuple[\"OrderedDict[str, torch.Tensor]\", Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load a checkpoint from `path`, extract a state_dict-ish mapping, and\n",
    "    return a FLAT dict of tensors keyed by full dotted paths, plus metadata.\n",
    "    \"\"\"\n",
    "    if _is_placeholder(path):\n",
    "        raise ValueError(\"CHECKPOINT_PATH is a placeholder; please provide a real checkpoint path before running surgery.\")\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {path}\")\n",
    "    checkpoint = _safe_load_any(path)\n",
    "    root = _extract_state_dict_root(checkpoint)\n",
    "    flat = _flatten_nested_state_dict(root)\n",
    "    canonical = OrderedDict((strip_known_prefixes(k), v) for k, v in sorted(flat.items()))\n",
    "    metadata = {}\n",
    "    if isinstance(checkpoint, (dict, OrderedDict)):\n",
    "        metadata = {k: v for k, v in checkpoint.items() if v is not root}\n",
    "    metadata = dict(metadata)\n",
    "    metadata.setdefault(\"original_keys\", len(root) if isinstance(root, (dict, OrderedDict)) else None)\n",
    "    metadata[\"flat_keys\"] = len(canonical)\n",
    "    return canonical, metadata\n",
    "\n",
    "\n",
    "_LAYER_PATTERNS = [\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:transformer\\.)?(?:h|layers|blocks|block|layer|encoder\\.layers|backbone\\.layers)\\.(\\d+)\\.(.+)$\",\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:encoder)\\.(?:layers)\\.(\\d+)\\.(.+)$\",\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:decoder)\\.(?:layers)\\.(\\d+)\\.(.+)$\",\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:gpt_neox)\\.(?:layers)\\.(\\d+)\\.(.+)$\",\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:transformer)\\.(?:blocks|h)\\.(\\d+)\\.(.+)$\",\n",
    "    r\"^(?:module\\.)?(?:model\\.)?(?:blocks|layers)\\.(\\d+)\\.(.+)$\",\n",
    "]\n",
    "\n",
    "def _match_with_patterns(key: str):\n",
    "    if CUSTOM_LAYER_REGEX:\n",
    "        custom_match = re.match(CUSTOM_LAYER_REGEX, key)\n",
    "        if custom_match:\n",
    "            idx = int(custom_match.group(1))\n",
    "            sub = custom_match.group(2)\n",
    "            return idx, sub\n",
    "    for pat in _LAYER_PATTERNS:\n",
    "        m = re.match(pat, key)\n",
    "        if m:\n",
    "            idx = int(m.group(1))\n",
    "            sub = m.group(2)\n",
    "            return idx, sub\n",
    "    return None\n",
    "\n",
    "def _fallback_numeric_segment(key: str):\n",
    "    parts = key.split(\".\")\n",
    "    for i, seg in enumerate(parts):\n",
    "        if seg.isdigit():\n",
    "            idx = int(seg)\n",
    "            sub = \".\".join(parts[i + 1 :]) if i + 1 < len(parts) else \"\"\n",
    "            return idx, sub\n",
    "    return None\n",
    "\n",
    "def discover_layer_schema(state_dict: Dict[str, torch.Tensor]) -> Tuple[List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Discover (layer_indices, layer_subkeys) from arbitrary key layouts.\n",
    "    Returns:\n",
    "        layer_indices: sorted list of ints\n",
    "        layer_subkeys: ordered list of subkey strings (canonical schema)\n",
    "    Behavior:\n",
    "        - Try several regexes; collect (idx, subkey) matches.\n",
    "        - If none match, use a generic fallback that finds the first numeric segment.\n",
    "        - Pick the most common subkey-set across layers as the canonical schema.\n",
    "    \"\"\"\n",
    "    layer_sets = defaultdict(set)\n",
    "    layer_orders = defaultdict(list)\n",
    "    matched = 0\n",
    "\n",
    "    for k in state_dict.keys():\n",
    "        res = _match_with_patterns(k)\n",
    "        if res:\n",
    "            matched += 1\n",
    "            idx, sub = res\n",
    "            layer_sets[idx].add(sub)\n",
    "            layer_orders[idx].append(sub)\n",
    "\n",
    "    if matched == 0:\n",
    "        for k in state_dict.keys():\n",
    "            res = _fallback_numeric_segment(k)\n",
    "            if res:\n",
    "                idx, sub = res\n",
    "                if sub:\n",
    "                    layer_sets[idx].add(sub)\n",
    "                    layer_orders[idx].append(sub)\n",
    "\n",
    "    if not layer_sets:\n",
    "        sample = list(state_dict.keys())[:50]\n",
    "        raise ValueError(\n",
    "            \"Could not discover layered parameters. Examples of keys:\\n  - \"\n",
    "            + \"\\n  - \".join(sample)\n",
    "        )\n",
    "\n",
    "    layer_indices = sorted(layer_sets.keys())\n",
    "\n",
    "    set_counts = Counter(frozenset(s) for s in layer_sets.values())\n",
    "    canonical_set = max(set_counts.items(), key=lambda kv: kv[1])[0]\n",
    "    canonical_list = sorted(list(canonical_set))\n",
    "\n",
    "    return layer_indices, canonical_list\n",
    "\n",
    "\n",
    "def infer_model_stats(state_dict: Dict[str, torch.Tensor], layer_indices: List[int]) -> Dict[str, Any]:\n",
    "    n_layers = max(layer_indices) + 1 if layer_indices else 0\n",
    "    norm_sizes: List[int] = []\n",
    "    one_d_sizes: List[int] = []\n",
    "    embed_shape: Optional[Tuple[int, ...]] = None\n",
    "    embed_key: Optional[str] = None\n",
    "    lm_head_shape: Optional[Tuple[int, ...]] = None\n",
    "    lm_head_key: Optional[str] = None\n",
    "    for key, tensor in state_dict.items():\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            continue\n",
    "        tensor = tensor.detach().cpu()\n",
    "        if tensor.ndim == 1:\n",
    "            size = int(tensor.shape[0])\n",
    "            one_d_sizes.append(size)\n",
    "            lower = key.lower()\n",
    "            if \"ln\" in lower or \"norm\" in lower:\n",
    "                norm_sizes.append(size)\n",
    "        lower_key = key.lower()\n",
    "        if embed_shape is None:\n",
    "            if (\n",
    "                \"emb.weight\" in lower_key\n",
    "                or lower_key.endswith(\"embedding.weight\")\n",
    "                or lower_key.endswith(\"embeddings.weight\")\n",
    "                or lower_key.endswith(\"wte.weight\")\n",
    "                or lower_key.endswith(\"tok_embeddings.weight\")\n",
    "                or lower_key.endswith(\"word_embeddings.weight\")\n",
    "            ):\n",
    "                embed_shape = tensor.shape\n",
    "                embed_key = key\n",
    "        if lm_head_shape is None and lower_key.endswith(\"lm_head.weight\"):\n",
    "            lm_head_shape = tensor.shape\n",
    "            lm_head_key = key\n",
    "    def _most_common(values: List[int]) -> Optional[int]:\n",
    "        if not values:\n",
    "            return None\n",
    "        counts = Counter(values)\n",
    "        return counts.most_common(1)[0][0]\n",
    "    d_model = _most_common(norm_sizes)\n",
    "    if d_model is None and embed_shape is not None and len(embed_shape) >= 2:\n",
    "        d_model = int(embed_shape[-1])\n",
    "    if d_model is None and lm_head_shape is not None and len(lm_head_shape) >= 2:\n",
    "        d_model = int(lm_head_shape[-1])\n",
    "    if d_model is None:\n",
    "        d_model = _most_common(one_d_sizes)\n",
    "    if d_model is None:\n",
    "        raise ValueError(\"Unable to infer d_model from the checkpoint tensors.\")\n",
    "    if embed_shape is None and lm_head_shape is not None:\n",
    "        vocab_size = int(lm_head_shape[0])\n",
    "        embed_key = lm_head_key\n",
    "    elif embed_shape is not None:\n",
    "        vocab_size = int(embed_shape[0])\n",
    "    else:\n",
    "        vocab_size = 50257\n",
    "    stats = {\n",
    "        \"n_layers\": n_layers,\n",
    "        \"d_model\": int(d_model),\n",
    "        \"vocab_size\": int(vocab_size),\n",
    "        \"embed_key\": embed_key,\n",
    "        \"lm_head_key\": lm_head_key,\n",
    "    }\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: shape scaling, Net2Net widening, and initialization helpers\n",
    "def compute_target_width(d_model: int, scale: float, round_mult: int) -> int:\n",
    "    widened = math.ceil(d_model * scale)\n",
    "    if round_mult > 1:\n",
    "        widened = int(math.ceil(widened / round_mult) * round_mult)\n",
    "    return int(max(d_model, widened))\n",
    "\n",
    "\n",
    "def scale_dim_like_dmodel(dim: int, d0: int, d1: int, tol: float = WIDTH_TOLERANCE, max_k: int = MAX_K_MATCH) -> Optional[int]:\n",
    "    if d0 <= 0 or dim <= 0:\n",
    "        return None\n",
    "    for k in range(1, max_k + 1):\n",
    "        target = k * d0\n",
    "        if abs(dim - target) <= max(int(round(target * tol)), 1):\n",
    "            return int(k * d1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_new_shape(shape: Tuple[int, ...], d0: int, d1: int) -> Tuple[Tuple[int, ...], bool]:\n",
    "    new_shape = list(shape)\n",
    "    changed = False\n",
    "    if len(shape) == 1:\n",
    "        scaled = scale_dim_like_dmodel(shape[0], d0, d1)\n",
    "        if scaled is not None and scaled != shape[0]:\n",
    "            new_shape[0] = scaled\n",
    "            changed = True\n",
    "    elif len(shape) == 2:\n",
    "        for axis in range(2):\n",
    "            scaled = scale_dim_like_dmodel(shape[axis], d0, d1)\n",
    "            if scaled is not None and scaled != shape[axis]:\n",
    "                new_shape[axis] = scaled\n",
    "                changed = True\n",
    "    return tuple(new_shape), changed\n",
    "\n",
    "\n",
    "def widen_tensor(tensor: torch.Tensor, new_shape: Tuple[int, ...]) -> torch.Tensor:\n",
    "    result = tensor.to(dtype=SURGERY_DTYPE, device=\"cpu\")\n",
    "    if tuple(result.shape) == new_shape:\n",
    "        return result.clone()\n",
    "    for axis, target_dim in enumerate(new_shape):\n",
    "        current_dim = result.shape[axis]\n",
    "        if current_dim == target_dim:\n",
    "            continue\n",
    "        if current_dim <= 0:\n",
    "            raise ValueError(f\"Cannot widen axis {axis} with size {current_dim} to {target_dim} for tensor.\")\n",
    "        if current_dim == 1:\n",
    "            expand_shape = list(result.shape)\n",
    "            expand_shape[axis] = target_dim\n",
    "            result = result.expand(*expand_shape).clone()\n",
    "        else:\n",
    "            indices = torch.linspace(0, current_dim - 1, target_dim, dtype=torch.float32)\n",
    "            indices = indices.round().clamp(0, current_dim - 1).to(torch.long)\n",
    "            result = torch.index_select(result, axis, indices)\n",
    "    if tuple(result.shape) != new_shape:\n",
    "        result = result.reshape(new_shape)\n",
    "    if tensor.numel() > 0 and tuple(tensor.shape) != new_shape:\n",
    "        std = tensor.float().std().item() if tensor.numel() > 1 else 0.0\n",
    "        jitter_std = 0.01 * std if std > 0 else 1e-3\n",
    "        if jitter_std > 0:\n",
    "            result = result + torch.randn_like(result) * jitter_std\n",
    "    return result\n",
    "\n",
    "\n",
    "def maybe_widen_tensor(key: str, tensor: torch.Tensor, d0: int, d1: int, target_shape: Optional[Tuple[int, ...]] = None) -> Tuple[torch.Tensor, str, Tuple[int, ...], Tuple[int, ...]]:\n",
    "    source = tensor.detach().cpu().to(dtype=SURGERY_DTYPE)\n",
    "    old_shape = tuple(source.shape)\n",
    "    if target_shape is None:\n",
    "        target_shape, changed = compute_new_shape(old_shape, d0, d1)\n",
    "    else:\n",
    "        changed = target_shape != old_shape\n",
    "    if not changed or target_shape is None:\n",
    "        return source.clone(), \"copied\", old_shape, old_shape\n",
    "    widened = widen_tensor(source, target_shape)\n",
    "    return widened, \"widened\", old_shape, target_shape\n",
    "\n",
    "\n",
    "def infer_fallback_shape(subkey: str, d1: int) -> Tuple[int, ...]:\n",
    "    lower = subkey.lower()\n",
    "    if \"weight\" in lower and (\"ln\" in lower or \"norm\" in lower):\n",
    "        return (d1,)\n",
    "    if \"bias\" in lower and (\"ln\" in lower or \"norm\" in lower):\n",
    "        return (d1,)\n",
    "    if \"bias\" in lower:\n",
    "        return (d1,)\n",
    "    if \"weight\" in lower:\n",
    "        return (d1, d1)\n",
    "    if \"scale\" in lower:\n",
    "        return ()\n",
    "    return (d1,)\n",
    "\n",
    "\n",
    "def synthesize_parameter(subkey: str, target_shape: Optional[Tuple[int, ...]], d1: int) -> torch.Tensor:\n",
    "    shape = target_shape or infer_fallback_shape(subkey, d1)\n",
    "    if len(shape) == 0:\n",
    "        value = 1e-3 if \"scale\" in subkey.lower() else 0.0\n",
    "        return torch.tensor(value, dtype=SURGERY_DTYPE)\n",
    "    tensor = torch.empty(shape, dtype=SURGERY_DTYPE)\n",
    "    lower = subkey.lower()\n",
    "    if len(shape) == 1:\n",
    "        if \"weight\" in lower and (\"ln\" in lower or \"norm\" in lower):\n",
    "            tensor.fill_(1.0)\n",
    "        elif \"bias\" in lower:\n",
    "            tensor.zero_()\n",
    "        elif \"scale\" in lower:\n",
    "            tensor.fill_(1e-3)\n",
    "        elif \"weight\" in lower:\n",
    "            nn_init.kaiming_uniform_(tensor.unsqueeze(0))\n",
    "            tensor.mul_(1e-3)\n",
    "        else:\n",
    "            tensor.zero_()\n",
    "    elif len(shape) == 2:\n",
    "        nn_init.kaiming_uniform_(tensor, a=math.sqrt(5))\n",
    "        tensor.mul_(1e-3)\n",
    "    else:\n",
    "        tensor.zero_()\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def build_new_state_dict(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    stats: Dict[str, Any],\n",
    "    layer_subkeys: List[str],\n",
    "    target_d_model: int,\n",
    ") -> Tuple[\"OrderedDict[str, torch.Tensor]\", Dict[str, List[str]], List[Tuple[str, Tuple[int, ...], Tuple[int, ...]]], List[Dict[str, Any]]]:\n",
    "    d0 = stats[\"d_model\"]\n",
    "    d1 = target_d_model\n",
    "    sd_new: \"OrderedDict[str, torch.Tensor]\" = OrderedDict()\n",
    "    operations: Dict[str, List[str]] = {\"copied\": [], \"widened\": [], \"synthesized\": []}\n",
    "    widen_samples: List[Tuple[str, Tuple[int, ...], Tuple[int, ...]]] = []\n",
    "    new_layer_info: List[Dict[str, Any]] = []\n",
    "    layer_values: Dict[int, Dict[str, torch.Tensor]] = defaultdict(dict)\n",
    "\n",
    "    for key, tensor in state_dict.items():\n",
    "        match = _match_with_patterns(key)\n",
    "        if not match:\n",
    "            match = _fallback_numeric_segment(key)\n",
    "        if match:\n",
    "            idx, subkey = match\n",
    "            if subkey:\n",
    "                layer_values[idx][subkey] = tensor\n",
    "                continue\n",
    "        new_tensor, op, old_shape, new_shape = maybe_widen_tensor(key, tensor, d0, d1)\n",
    "        sd_new[key] = new_tensor\n",
    "        operations[op].append(key)\n",
    "        if op == \"widened\" and len(widen_samples) < 5:\n",
    "            widen_samples.append((key, old_shape, new_shape))\n",
    "\n",
    "    layer_target_shapes: Dict[str, Optional[Tuple[int, ...]]] = {}\n",
    "    for subkey in layer_subkeys:\n",
    "        sample_tensor = None\n",
    "        for idx in range(stats[\"n_layers\"]):\n",
    "            candidate = layer_values.get(idx, {}).get(subkey)\n",
    "            if candidate is not None:\n",
    "                sample_tensor = candidate\n",
    "                break\n",
    "        if sample_tensor is not None:\n",
    "            shape, _ = compute_new_shape(tuple(sample_tensor.shape), d0, d1)\n",
    "        else:\n",
    "            shape = infer_fallback_shape(subkey, d1)\n",
    "        layer_target_shapes[subkey] = shape\n",
    "\n",
    "    for idx in range(stats[\"n_layers\"]):\n",
    "        values = layer_values.get(idx, {})\n",
    "        for subkey in layer_subkeys:\n",
    "            source_tensor = values.get(subkey)\n",
    "            if source_tensor is None:\n",
    "                continue\n",
    "            key = f\"layers.{idx}.{subkey}\"\n",
    "            target_shape = layer_target_shapes.get(subkey)\n",
    "            new_tensor, op, old_shape, new_shape = maybe_widen_tensor(key, source_tensor, d0, d1, target_shape)\n",
    "            sd_new[key] = new_tensor\n",
    "            operations[op].append(key)\n",
    "            if op == \"widened\" and len(widen_samples) < 5:\n",
    "                widen_samples.append((key, old_shape, new_shape))\n",
    "\n",
    "    total_new_layers = ADD_CLASSIC_LAY + ADD_LIQUID_LAY\n",
    "    for offset in range(total_new_layers):\n",
    "        layer_idx = stats[\"n_layers\"] + offset\n",
    "        layer_type = \"classic\" if offset < ADD_CLASSIC_LAY else \"liquid\"\n",
    "        new_layer_info.append({\"index\": layer_idx, \"type\": layer_type})\n",
    "        for subkey in layer_subkeys:\n",
    "            key = f\"layers.{layer_idx}.{subkey}\"\n",
    "            target_shape = layer_target_shapes.get(subkey)\n",
    "            synthesized = synthesize_parameter(subkey, target_shape, d1)\n",
    "            sd_new[key] = synthesized\n",
    "            operations[\"synthesized\"].append(key)\n",
    "\n",
    "    return sd_new, operations, widen_samples, new_layer_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Cell 5: artifact writers, reporting, and utility helpers\n",
    "    def build_layer_types(n_layers: int) -> List[str]:\n",
    "        return [\"liquid\"] * n_layers + [\"classic\"] * ADD_CLASSIC_LAY + [\"liquid\"] * ADD_LIQUID_LAY\n",
    "\n",
    "\n",
    "    def build_freeze_mask(n_layers: int) -> Dict[str, bool]:\n",
    "        total_layers = n_layers + ADD_CLASSIC_LAY + ADD_LIQUID_LAY\n",
    "        mask: Dict[str, bool] = {}\n",
    "        for idx in range(total_layers):\n",
    "            freeze = n_layers <= idx < n_layers + ADD_CLASSIC_LAY\n",
    "            mask[f\"layers.{idx}\"] = bool(freeze)\n",
    "        return mask\n",
    "\n",
    "\n",
    "    def save_artifacts(\n",
    "        sd_new: \"OrderedDict[str, torch.Tensor]\",\n",
    "        stats: Dict[str, Any],\n",
    "        target_d_model: int,\n",
    "        layer_types: List[str],\n",
    "        operations: Dict[str, List[str]],\n",
    "        new_layer_info: List[Dict[str, Any]],\n",
    "        widen_samples: List[Tuple[str, Tuple[int, ...], Tuple[int, ...]]],\n",
    "        metadata: Dict[str, Any],\n",
    "    ) -> Dict[str, str]:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        checkpoint_path = os.path.join(OUTPUT_DIR, \"stage1_surgery.pt\")\n",
    "        state_to_save = OrderedDict((k, v.detach().cpu().to(dtype=SURGERY_DTYPE)) for k, v in sd_new.items())\n",
    "        torch.save({\"state_dict\": state_to_save, \"note\": \"Stage-1 surgery checkpoint\"}, checkpoint_path)\n",
    "        config = {\n",
    "            \"d_model_old\": stats[\"d_model\"],\n",
    "            \"d_model_new\": target_d_model,\n",
    "            \"n_layers_old\": stats[\"n_layers\"],\n",
    "            \"n_layers_new\": stats[\"n_layers\"] + ADD_CLASSIC_LAY + ADD_LIQUID_LAY,\n",
    "            \"vocab_size\": stats[\"vocab_size\"],\n",
    "            \"layer_types\": layer_types,\n",
    "            \"max_seq_len\": DEFAULT_MAX_SEQ,\n",
    "        }\n",
    "        config_path = os.path.join(OUTPUT_DIR, \"config_stage1.json\")\n",
    "        with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        freeze_mask = build_freeze_mask(stats[\"n_layers\"])\n",
    "        freeze_path = os.path.join(OUTPUT_DIR, \"freeze_mask.json\")\n",
    "        with open(freeze_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(freeze_mask, f, indent=2)\n",
    "        copied_count = len(set(operations.get(\"copied\", [])))\n",
    "        widened_count = len(set(operations.get(\"widened\", [])))\n",
    "        synthesized_count = len(set(operations.get(\"synthesized\", [])))\n",
    "        report_lines = [\n",
    "            \"# Stage-1 Surgery Report\",\n",
    "            \"\",\n",
    "            \"## Source\",\n",
    "            f\"- Checkpoint path: {CHECKPOINT_PATH}\",\n",
    "            f\"- Inferred layers: {stats['n_layers']}\",\n",
    "            f\"- Inferred d_model: {stats['d_model']}\",\n",
    "            f\"- Inferred vocab size: {stats['vocab_size']}\",\n",
    "            \"\",\n",
    "            \"## Target\",\n",
    "            f\"- Target d_model: {target_d_model}\",\n",
    "            f\"- Total layers after surgery: {stats['n_layers'] + ADD_CLASSIC_LAY + ADD_LIQUID_LAY}\",\n",
    "            f\"- Added classic layers: {ADD_CLASSIC_LAY}\",\n",
    "            f\"- Added liquid layers: {ADD_LIQUID_LAY}\",\n",
    "            \"\",\n",
    "            \"## Operations\",\n",
    "            f\"- Copied tensors: {copied_count}\",\n",
    "            f\"- Widened tensors: {widened_count}\",\n",
    "            f\"- Synthesized tensors: {synthesized_count}\",\n",
    "            \"\",\n",
    "            \"## New layers\",\n",
    "        ]\n",
    "        for info in new_layer_info:\n",
    "            report_lines.append(f\"- layers.{info['index']}: {info['type']}\")\n",
    "        if widen_samples:\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"## Sample widened tensors\")\n",
    "            for key, old_shape, new_shape in widen_samples:\n",
    "                report_lines.append(f\"- {key}: {old_shape} -> {new_shape}\")\n",
    "        if metadata:\n",
    "            report_lines.append(\"\")\n",
    "            report_lines.append(\"## Checkpoint metadata keys\")\n",
    "            for meta_key in sorted(metadata.keys()):\n",
    "                report_lines.append(f\"- {meta_key}\")\n",
    "        report_lines.append(\"\")\n",
    "        report_lines.append(f\"Generated on {datetime.datetime.utcnow().isoformat()}Z\")\n",
    "        report_path = os.path.join(OUTPUT_DIR, \"surgery_report.md\")\n",
    "        with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\n",
    "\".join(report_lines))\n",
    "        return {\n",
    "            \"checkpoint\": checkpoint_path,\n",
    "            \"config\": config_path,\n",
    "            \"freeze_mask\": freeze_path,\n",
    "            \"report\": report_path,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: run the Stage-1 surgery end-to-end\n",
    "def run_stage1_surgery() -> Optional[Dict[str, Any]]:\n",
    "    if _is_placeholder(CHECKPOINT_PATH):\n",
    "        print(\"CHECKPOINT_PATH is a placeholder. Update it to run surgery.\")\n",
    "        return None\n",
    "    start_time = time.time()\n",
    "    state_dict, metadata = load_stage0_state_dict(CHECKPOINT_PATH)\n",
    "    layer_indices, layer_subkeys = discover_layer_schema(state_dict)\n",
    "    if not layer_indices:\n",
    "        print(\"Layer discovery returned no layer indices. Showing first 50 parameter keys:\")\n",
    "        for key in list(state_dict.keys())[:50]:\n",
    "            print(f\"  - {key}\")\n",
    "        raise ValueError(\"Layer discovery did not find any layer indices.\")\n",
    "    earliest_idx = layer_indices[0]\n",
    "    example_keys: List[str] = []\n",
    "    for key in state_dict.keys():\n",
    "        match = _match_with_patterns(key)\n",
    "        if not match:\n",
    "            match = _fallback_numeric_segment(key)\n",
    "        if match and match[0] == earliest_idx:\n",
    "            example_keys.append(key)\n",
    "            if len(example_keys) >= 10:\n",
    "                break\n",
    "    print(f\"Layer subkeys (first 10): {layer_subkeys[:10]}\")\n",
    "    if example_keys:\n",
    "        print(f\"Example parameter keys for layer index {earliest_idx} (first {len(example_keys)}):\")\n",
    "        for key in example_keys:\n",
    "            print(f\"  - {key}\")\n",
    "    else:\n",
    "        print(f\"No parameter keys found for layer index {earliest_idx} using detected patterns.\")\n",
    "    stats = infer_model_stats(state_dict, layer_indices)\n",
    "    stats[\"layer_subkeys\"] = layer_subkeys\n",
    "    target_d_model = compute_target_width(stats[\"d_model\"], WIDTH_SCALE, ROUND_MULT)\n",
    "    sd_new, operations, widen_samples, new_layer_info = build_new_state_dict(\n",
    "        state_dict, stats, layer_subkeys, target_d_model\n",
    "    )\n",
    "    layer_types = build_layer_types(stats[\"n_layers\"])\n",
    "    artifact_paths = save_artifacts(\n",
    "        sd_new,\n",
    "        stats,\n",
    "        target_d_model,\n",
    "        layer_types,\n",
    "        operations,\n",
    "        new_layer_info,\n",
    "        widen_samples,\n",
    "        metadata,\n",
    "    )\n",
    "    duration = time.time() - start_time\n",
    "    op_counts = {name: len(set(keys)) for name, keys in operations.items()}\n",
    "    print(\"--- Stage-0 inference ---\")\n",
    "    print(f\"n_layers:   {stats['n_layers']}\")\n",
    "    print(f\"d_model:    {stats['d_model']}\")\n",
    "    print(f\"vocab_size: {stats['vocab_size']}\")\n",
    "    print(f\"layer schema keys: {len(layer_subkeys)} entries\")\n",
    "    print(\"--- Stage-1 summary ---\")\n",
    "    print(f\"target d_model: {target_d_model}\")\n",
    "    print(f\"added classic layers: {ADD_CLASSIC_LAY}\")\n",
    "    print(f\"added liquid layers:  {ADD_LIQUID_LAY}\")\n",
    "    print(f\"copied tensors:      {op_counts.get('copied', 0)}\")\n",
    "    print(f\"widened tensors:     {op_counts.get('widened', 0)}\")\n",
    "    print(f\"synthesized tensors: {op_counts.get('synthesized', 0)}\")\n",
    "    print(f\"artifacts: {json.dumps(artifact_paths, indent=2)}\")\n",
    "    print(f\"elapsed: {duration:.2f}s\")\n",
    "    return {\n",
    "        \"state_dict\": sd_new,\n",
    "        \"stats\": stats,\n",
    "        \"target_d_model\": target_d_model,\n",
    "        \"layer_types\": layer_types,\n",
    "        \"operations\": operations,\n",
    "        \"operation_counts\": op_counts,\n",
    "        \"widen_samples\": widen_samples,\n",
    "        \"new_layer_info\": new_layer_info,\n",
    "        \"artifacts\": artifact_paths,\n",
    "        \"metadata\": metadata,\n",
    "    }\n",
    "\n",
    "\n",
    "results = run_stage1_surgery()\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Cell 7: inspect the synthesized checkpoint\n",
    "        if isinstance(results, dict) and results.get(\"state_dict\") is not None:\n",
    "            sd_preview = results[\"state_dict\"]\n",
    "            all_keys = list(sd_preview.keys())\n",
    "            print(\"First 10 keys:\")\n",
    "            for key in all_keys[:10]:\n",
    "                print(\" \", key)\n",
    "            widen_samples = results.get(\"widen_samples\", [])\n",
    "            if widen_samples:\n",
    "                print(\"\n",
    "Sample widened tensors:\")\n",
    "                for key, old_shape, new_shape in widen_samples[:3]:\n",
    "                    print(f\" - {key}: {old_shape} -> {new_shape}\")\n",
    "            op_counts = results.get(\"operation_counts\", {})\n",
    "            print(\"\n",
    "Operation counts:\")\n",
    "            for name in sorted(op_counts.keys()):\n",
    "                print(f\" - {name}: {op_counts[name]}\")\n",
    "        else:\n",
    "            print(\"No results available \u2014 set CHECKPOINT_PATH and rerun the surgery cell.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}