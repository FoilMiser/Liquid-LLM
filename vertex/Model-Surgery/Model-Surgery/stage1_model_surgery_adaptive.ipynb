{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stage-1 Model Surgery (Adaptive) \u2014 Liquid \u2192 Hybrid (Net2Net widen + new layers)\n",
        "\n",
        "This notebook:\n",
        "- Introspects your student checkpoint to infer d_model, n_layers, and vocab size.\n",
        "- Widens all tensors that depend on d_model by +10% (rounded to a multiple of 8).\n",
        "- Appends 2 classic (transformer-style) layers (tiny-out init) and 3 liquid layers (small-scale init).\n",
        "- Writes out: a new checkpoint, config_stage1.json, freeze_mask.json, and surgery_report.md.\n",
        "\n",
        "> Fill in the two placeholders for CHECKPOINT_PATH and OUTPUT_DIR below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: install (optional if you've already installed requirements.txt)\n",
        "# This uses the PyTorch cu121 wheel index.\n",
        "# !pip install -r requirements.txt --index-url https://download.pytorch.org/whl/cu121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: imports, config, and IO paths\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import shutil\n",
        "import subprocess\n",
        "import datetime\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "SURGERY_DTYPE = torch.float32\n",
        "\n",
        "# ---------------------------\n",
        "# TODO: set your IO paths\n",
        "# ---------------------------\n",
        "CHECKPOINT_PATH = r\"C:\\Users\\samsf\\Liquid-LLM\\Important-Model-Checkpoint\\stage0.pt\"\n",
        "OUTPUT_DIR = r\"C:\\Users\\samsf\\Liquid-LLM\\Important-Model-Checkpoint\\stage-1\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Surgery hyperparameters / knobs\n",
        "WIDTH_SCALE      = 1.10   # +10%\n",
        "ROUND_MULT       = 8      # tensor-core friendly multiple\n",
        "ADD_CLASSIC_LAY  = 2\n",
        "ADD_LIQUID_LAY   = 3\n",
        "WIDTH_TOLERANCE  = 0.06   # tolerate +/-6% when matching k*d_model dims\n",
        "ENABLE_DRY_RUN   = False  # set True to verify widen-only logits similarity\n",
        "DRY_RUN_BATCH    = 2\n",
        "DRY_RUN_SEQ      = 16\n",
        "DRY_RUN_COS_TGT  = 0.99\n",
        "N_HEADS_OVERRIDE: Optional[int] = None  # set to int to override inference\n",
        "\n",
        "# Dtype override (default float32). For bf16/fp16 surgery, adjust SURGERY_DTYPE above.\n",
        "\n",
        "# Optional: add any package roots so imports resolve inside the notebook runtime.\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PACKAGE_HINTS = [\n",
        "    Path(\"vertex\") / \"package\" / \"Stage_1\",\n",
        "    Path(\"vertex\") / \"package\" / \"liquid_llm_vertex_pkg_4\" / \"src\",\n",
        "    Path(\"vertex\") / \"package\" / \"liquid_llm_vertex_pkg_3\" / \"src\",\n",
        "    Path(\"vertex\") / \"package\" / \"liquid_llm_vertex_pkg_4_annealing\" / \"src\",\n",
        "    Path(\"vertex\") / \"package\" / \"liquid_llm_vertex_pkg_1024\" / \"src\",\n",
        "    Path(\"vertex\") / \"package\" / \"liquid_llm_vertex_pkg_1024_next\" / \"src\",\n",
        "]\n",
        "\n",
        "added_sys_paths: List[str] = []\n",
        "search_roots = [NOTEBOOK_DIR, *NOTEBOOK_DIR.parents]\n",
        "for rel_path in PACKAGE_HINTS:\n",
        "    for base in search_roots:\n",
        "        candidate = (base / rel_path).resolve()\n",
        "        if candidate.is_dir():\n",
        "            candidate_str = str(candidate)\n",
        "            if candidate_str not in sys.path:\n",
        "                sys.path.append(candidate_str)\n",
        "                added_sys_paths.append(candidate_str)\n",
        "            break\n",
        "if added_sys_paths:\n",
        "    print(\"Added package roots:\")\n",
        "    for _path in added_sys_paths:\n",
        "        print(f\"- {_path}\")\n",
        "else:\n",
        "    print(\"WARNING: no package roots were added automatically; adjust PACKAGE_HINTS if imports fail.\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Import the real model/config classes from the package\n",
        "# ------------------------------------------------------------------\n",
        "from Stage_1.models.config import ModelConfig\n",
        "from Stage_1.models.blocks import LiquidBlock, ClassicBlock\n",
        "from Stage_1.models.stage1_model import Stage1Model\n",
        "\n",
        "try:  # optional Stage-0 student class (for dry-run cosine check if available)\n",
        "    from liquid_llm.models.liquid import StudentLM\n",
        "except Exception:\n",
        "    StudentLM = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: checkpoint IO + dimension inference utilities\n",
        "PREFIXES_TO_STRIP = (\n",
        "    \"module.\",\n",
        "    \"model.\",\n",
        "    \"student.\",\n",
        "    \"state_dict.\",\n",
        "    \"network.\",\n",
        ")\n",
        "\n",
        "\n",
        "def strip_known_prefixes(key: str) -> str:\n",
        "    new_key = key\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for prefix in PREFIXES_TO_STRIP:\n",
        "            if new_key.startswith(prefix):\n",
        "                new_key = new_key[len(prefix) :]\n",
        "                changed = True\n",
        "    return new_key\n",
        "\n",
        "\n",
        "def resolve_checkpoint_path(path: str) -> str:\n",
        "    if not path.startswith(\"gs://\"):\n",
        "        return path\n",
        "    local_name = os.path.join(\"/tmp\", os.path.basename(path))\n",
        "    if shutil.which(\"gsutil\"):\n",
        "        print(f\"Copying {path} -> {local_name} via gsutil\")\n",
        "        subprocess.run([\"gsutil\", \"cp\", path, local_name], check=True)\n",
        "        return local_name\n",
        "    try:\n",
        "        import gcsfs  # type: ignore\n",
        "\n",
        "        print(f\"Copying {path} -> {local_name} via gcsfs\")\n",
        "        fs = gcsfs.GCSFileSystem()\n",
        "        with fs.open(path, \"rb\") as src, open(local_name, \"wb\") as dst:\n",
        "            dst.write(src.read())\n",
        "        return local_name\n",
        "    except Exception as exc:  # pragma: no cover - optional dependency path\n",
        "        raise RuntimeError(\n",
        "            \"gs:// path provided but neither gsutil nor gcsfs is available.\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "def canonicalize_state_dict(\n",
        "    state_dict: Dict[str, torch.Tensor]\n",
        ") -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "    canonical: Dict[str, torch.Tensor] = {}\n",
        "    aux: Dict[str, Any] = {}\n",
        "    for key, value in state_dict.items():\n",
        "        if torch.is_tensor(value):\n",
        "            canonical[strip_known_prefixes(key)] = value.detach().to(SURGERY_DTYPE)\n",
        "        else:\n",
        "            aux[key] = value\n",
        "    return canonical, aux\n",
        "\n",
        "\n",
        "def load_checkpoint(path: str) -> Tuple[Dict[str, torch.Tensor], Dict[str, Any]]:\n",
        "    resolved = resolve_checkpoint_path(path)\n",
        "    print(f\"Loading checkpoint from {resolved}\")\n",
        "    payload = torch.load(resolved, map_location=\"cpu\")\n",
        "    if isinstance(payload, dict):\n",
        "        if \"state_dict\" in payload and isinstance(payload[\"state_dict\"], dict):\n",
        "            state_dict = payload[\"state_dict\"]\n",
        "            metadata = {k: v for k, v in payload.items() if k != \"state_dict\"}\n",
        "        else:\n",
        "            state_dict = {k: v for k, v in payload.items() if torch.is_tensor(v)}\n",
        "            metadata = {k: v for k, v in payload.items() if not torch.is_tensor(v)}\n",
        "    elif isinstance(payload, torch.Tensor):\n",
        "        raise ValueError(\"Expected a dict-like checkpoint payload, found a raw tensor.\")\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported checkpoint format; expected a mapping or state_dict wrapper.\")\n",
        "    canonical, aux = canonicalize_state_dict(state_dict)\n",
        "    if aux:\n",
        "        metadata.setdefault(\"non_tensor\", {}).update(aux)\n",
        "    return canonical, metadata\n",
        "\n",
        "\n",
        "def infer_layer_count(keys: Iterable[str]) -> int:\n",
        "    patterns = [\n",
        "        re.compile(r\"(?:^|\\.)(layers|blocks|h)\\.(\\d+)\")\n",
        "    ]\n",
        "    indices: set[int] = set()\n",
        "    for key in keys:\n",
        "        for pat in patterns:\n",
        "            for match in pat.finditer(key):\n",
        "                try:\n",
        "                    indices.add(int(match.group(2)))\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return (max(indices) + 1) if indices else 0\n",
        "\n",
        "\n",
        "def infer_vocab_and_width(\n",
        "    state_dict: Dict[str, torch.Tensor]\n",
        ") -> Tuple[int, int, str]:\n",
        "    candidates: List[Tuple[int, int, str, torch.Tensor]] = []\n",
        "    for key, tensor in state_dict.items():\n",
        "        if tensor.ndim != 2:\n",
        "            continue\n",
        "        vocab, width = tensor.shape\n",
        "        score = 0\n",
        "        key_lower = key.lower()\n",
        "        if vocab < 128 or width < 32:\n",
        "            continue\n",
        "        if \"embed\" in key_lower:\n",
        "            score += 5\n",
        "        if \"token\" in key_lower:\n",
        "            score += 2\n",
        "        if \"lm_head\" in key_lower or \"output\" in key_lower:\n",
        "            score += 1\n",
        "        candidates.append((score, -vocab, key, tensor))\n",
        "    if not candidates:\n",
        "        raise ValueError(\"Could not infer embedding matrix to determine d_model/vocab_size.\")\n",
        "    candidates.sort(reverse=True)\n",
        "    _, _, key, tensor = candidates[0]\n",
        "    vocab, width = tensor.shape\n",
        "    return width, vocab, key\n",
        "\n",
        "\n",
        "def infer_max_seq_len(state_dict: Dict[str, torch.Tensor]) -> int:\n",
        "    candidates: List[Tuple[int, str, torch.Tensor]] = []\n",
        "    for key, tensor in state_dict.items():\n",
        "        lower = key.lower()\n",
        "        if \"pos\" not in lower:\n",
        "            continue\n",
        "        if tensor.ndim == 2:\n",
        "            length = tensor.shape[0]\n",
        "        elif tensor.ndim >= 3:\n",
        "            length = tensor.shape[-2]\n",
        "        else:\n",
        "            continue\n",
        "        candidates.append((length, key, tensor))\n",
        "    if not candidates:\n",
        "        return 2048\n",
        "    candidates.sort(reverse=True)\n",
        "    length, key, tensor = candidates[0]\n",
        "    print(f\"Detected positional embedding candidate '{key}' with length={length}\")\n",
        "    return int(length)\n",
        "\n",
        "\n",
        "def _search_heads_in_metadata(obj: Any) -> Optional[int]:\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            key_l = key.lower()\n",
        "            if key_l in {\"n_heads\", \"num_heads\", \"num_attention_heads\", \"n_head\"}:\n",
        "                if isinstance(value, (int, float)):\n",
        "                    return int(value)\n",
        "            result = _search_heads_in_metadata(value)\n",
        "            if result is not None:\n",
        "                return result\n",
        "    return None\n",
        "\n",
        "\n",
        "def infer_n_heads(\n",
        "    state_dict: Dict[str, torch.Tensor], metadata: Dict[str, Any], d_model: int\n",
        ") -> Optional[int]:\n",
        "    heads = _search_heads_in_metadata(metadata)\n",
        "    if heads:\n",
        "        return heads\n",
        "    for key, tensor in state_dict.items():\n",
        "        lower = key.lower()\n",
        "        if tensor.numel() == 1 and (\"num_heads\" in lower or \"n_heads\" in lower):\n",
        "            return int(tensor.item())\n",
        "    # Heuristic fallback: prefer divisors producing head_dim close to 64\n",
        "    divisors = [h for h in range(1, d_model + 1) if d_model % h == 0]\n",
        "    if not divisors:\n",
        "        return None\n",
        "    preferred = [16, 12, 8, 4, 32]\n",
        "    for candidate in preferred:\n",
        "        if candidate in divisors:\n",
        "            return candidate\n",
        "    return max(divisors)\n",
        "\n",
        "\n",
        "def infer_model_stats(\n",
        "    state_dict: Dict[str, torch.Tensor], metadata: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    d_model, vocab_size, embed_key = infer_vocab_and_width(state_dict)\n",
        "    n_layers = infer_layer_count(state_dict.keys())\n",
        "    max_seq_len = infer_max_seq_len(state_dict)\n",
        "    n_heads = infer_n_heads(state_dict, metadata, d_model)\n",
        "    info = {\n",
        "        \"d_model\": d_model,\n",
        "        \"vocab_size\": vocab_size,\n",
        "        \"embed_key\": embed_key,\n",
        "        \"n_layers\": n_layers,\n",
        "        \"max_seq_len\": max_seq_len,\n",
        "        \"n_heads\": n_heads,\n",
        "    }\n",
        "    return info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: shape scaling, Net2Net widening, and initialization helpers\n",
        "def compute_target_width(\n",
        "    d_model: int,\n",
        "    scale: float,\n",
        "    round_mult: int,\n",
        "    n_heads: Optional[int] = None,\n",
        ") -> int:\n",
        "    widened = int(round(d_model * scale / round_mult) * round_mult)\n",
        "    widened = max(widened, d_model)\n",
        "    if n_heads and n_heads > 0:\n",
        "        remainder = widened % n_heads\n",
        "        if remainder:\n",
        "            widened += n_heads - remainder\n",
        "    return widened\n",
        "\n",
        "\n",
        "def scale_dim(dim: int, d_old: int, d_new: int, tol: float) -> int:\n",
        "    if d_old == 0:\n",
        "        return dim\n",
        "    for k in range(1, 9):\n",
        "        baseline = k * d_old\n",
        "        if baseline == 0:\n",
        "            continue\n",
        "        if abs(dim - baseline) <= tol * baseline:\n",
        "            return int(round(k * d_new))\n",
        "    if abs(dim - d_old) <= tol * max(dim, d_old):\n",
        "        return int(round(dim * d_new / d_old))\n",
        "    return dim\n",
        "\n",
        "\n",
        "def inverse_scale_dim(dim: int, d_old: int, d_new: int, tol: float) -> int:\n",
        "    if d_new == 0:\n",
        "        return dim\n",
        "    for k in range(1, 9):\n",
        "        baseline = k * d_new\n",
        "        if baseline == 0:\n",
        "            continue\n",
        "        if abs(dim - baseline) <= tol * baseline:\n",
        "            return int(round(k * d_old))\n",
        "    if abs(dim - d_new) <= tol * max(dim, d_new):\n",
        "        return int(round(dim * d_old / d_new))\n",
        "    return dim\n",
        "\n",
        "\n",
        "def scale_shape(shape: Tuple[int, ...], d_old: int, d_new: int, tol: float) -> Tuple[int, ...]:\n",
        "    return tuple(scale_dim(dim, d_old, d_new, tol) for dim in shape)\n",
        "\n",
        "\n",
        "def inverse_scale_shape(\n",
        "    shape: Iterable[int], d_old: int, d_new: int, tol: float\n",
        ") -> Tuple[int, ...]:\n",
        "    return tuple(inverse_scale_dim(dim, d_old, d_new, tol) for dim in shape)\n",
        "\n",
        "\n",
        "def key_similarity(dest_key: str, src_key: str) -> float:\n",
        "    seq_score = SequenceMatcher(None, dest_key, src_key).ratio()\n",
        "    dest_tokens = set(filter(None, re.split(r\"[._]\", dest_key)))\n",
        "    src_tokens = set(filter(None, re.split(r\"[._]\", src_key)))\n",
        "    token_score = 0.0\n",
        "    if dest_tokens or src_tokens:\n",
        "        token_score = len(dest_tokens & src_tokens) / max(len(dest_tokens | src_tokens), 1)\n",
        "    return seq_score + token_score\n",
        "\n",
        "\n",
        "def is_new_layer_key(key: str, base_layers: int) -> bool:\n",
        "    if not key.startswith(\"blocks.\"):\n",
        "        return False\n",
        "    parts = key.split(\".\")\n",
        "    if len(parts) < 2 or not parts[1].isdigit():\n",
        "        return False\n",
        "    return int(parts[1]) >= base_layers\n",
        "\n",
        "\n",
        "def build_parameter_mapping(\n",
        "    dest_state: Dict[str, torch.Tensor],\n",
        "    src_state: Dict[str, torch.Tensor],\n",
        "    base_layers: int,\n",
        "    d_old: int,\n",
        "    d_new: int,\n",
        "    tol: float,\n",
        ") -> Dict[str, str]:\n",
        "    mapping: Dict[str, str] = {}\n",
        "    assigned: set[str] = set()\n",
        "    for dest_key, dest_tensor in dest_state.items():\n",
        "        if is_new_layer_key(dest_key, base_layers):\n",
        "            continue\n",
        "        expected_old_shape = inverse_scale_shape(dest_tensor.shape, d_old, d_new, tol)\n",
        "        candidates: List[Tuple[float, str]] = []\n",
        "        for src_key, src_tensor in src_state.items():\n",
        "            if src_key in assigned:\n",
        "                continue\n",
        "            if src_tensor.shape == expected_old_shape:\n",
        "                candidates.append((key_similarity(dest_key, src_key), src_key))\n",
        "            elif (\n",
        "                len(dest_tensor.shape) == len(expected_old_shape) + 1\n",
        "                and dest_tensor.shape[0] == 1\n",
        "                and src_tensor.shape == expected_old_shape[1:]\n",
        "            ):\n",
        "                candidates.append((key_similarity(dest_key, src_key), src_key))\n",
        "        if candidates:\n",
        "            candidates.sort(reverse=True)\n",
        "            best_key = candidates[0][1]\n",
        "            mapping[dest_key] = best_key\n",
        "            assigned.add(best_key)\n",
        "    return mapping\n",
        "\n",
        "\n",
        "def widen_tensor(src: torch.Tensor, target_shape: Tuple[int, ...]) -> torch.Tensor:\n",
        "    tensor = src.detach().to(SURGERY_DTYPE)\n",
        "    if tensor.shape == target_shape:\n",
        "        return tensor.clone()\n",
        "    result = tensor\n",
        "    changed = False\n",
        "    for dim, new_size in enumerate(target_shape):\n",
        "        old_size = result.shape[dim]\n",
        "        if old_size == new_size:\n",
        "            continue\n",
        "        changed = True\n",
        "        if old_size == 0:\n",
        "            raise ValueError(f\"Cannot widen dimension {dim} with size 0\")\n",
        "        indices = torch.linspace(0, old_size - 1, new_size)\n",
        "        indices = indices.round().long()\n",
        "        result = torch.index_select(result, dim, indices)\n",
        "    result = result.clone()\n",
        "    if changed:\n",
        "        std = tensor.std().item() if tensor.numel() else 0.0\n",
        "        noise_std = max(abs(std) * 0.01, 1e-3)\n",
        "        noise = torch.randn(result.shape, dtype=result.dtype)\n",
        "        result.add_(noise * noise_std)\n",
        "    return result\n",
        "\n",
        "\n",
        "def adapt_tensor(\n",
        "    src_tensor: torch.Tensor,\n",
        "    dest_shape: Tuple[int, ...],\n",
        "    d_old: int,\n",
        "    d_new: int,\n",
        "    tol: float,\n",
        ") -> Tuple[Optional[torch.Tensor], str]:\n",
        "    tensor = src_tensor.detach().to(SURGERY_DTYPE)\n",
        "    if tensor.shape == dest_shape:\n",
        "        return tensor.clone(), \"copy\"\n",
        "    # Allow unsqueeze on leading dimension\n",
        "    if len(dest_shape) == tensor.ndim + 1 and dest_shape[0] == 1:\n",
        "        tensor = tensor.unsqueeze(0)\n",
        "    scaled_shape = scale_shape(tensor.shape, d_old, d_new, tol)\n",
        "    if tuple(dest_shape) == tuple(scaled_shape):\n",
        "        widened = widen_tensor(tensor, dest_shape)\n",
        "        status = \"copy\" if tensor.shape == dest_shape else \"widen\"\n",
        "        return widened, status\n",
        "    return None, \"skip\"\n",
        "\n",
        "\n",
        "def transplant_weights(\n",
        "    src_state: Dict[str, torch.Tensor],\n",
        "    dest_state: Dict[str, torch.Tensor],\n",
        "    base_layers: int,\n",
        "    d_old: int,\n",
        "    d_new: int,\n",
        "    tol: float,\n",
        ") -> Tuple[Dict[str, torch.Tensor], List[str], List[str], List[str], Dict[str, str]]:\n",
        "    mapping = build_parameter_mapping(dest_state, src_state, base_layers, d_old, d_new, tol)\n",
        "    updated = dict(dest_state)\n",
        "    copied: List[str] = []\n",
        "    widened: List[str] = []\n",
        "    skipped: List[str] = []\n",
        "    for dest_key, dest_tensor in dest_state.items():\n",
        "        if is_new_layer_key(dest_key, base_layers):\n",
        "            skipped.append(dest_key)\n",
        "            continue\n",
        "        src_key = mapping.get(dest_key)\n",
        "        if src_key is None:\n",
        "            skipped.append(dest_key)\n",
        "            continue\n",
        "        src_tensor = src_state[src_key]\n",
        "        adapted, status = adapt_tensor(src_tensor, tuple(dest_tensor.shape), d_old, d_new, tol)\n",
        "        if adapted is None:\n",
        "            skipped.append(dest_key)\n",
        "            continue\n",
        "        updated[dest_key] = adapted\n",
        "        if status == \"copy\":\n",
        "            copied.append(dest_key)\n",
        "        else:\n",
        "            widened.append(dest_key)\n",
        "    return updated, copied, widened, skipped, mapping\n",
        "\n",
        "\n",
        "def init_classic_block(block: nn.Module) -> None:\n",
        "    for module in block.modules():\n",
        "        if isinstance(module, nn.LayerNorm):\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=1e-3)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.MultiheadAttention):\n",
        "            nn.init.normal_(module.in_proj_weight, mean=0.0, std=1e-3)\n",
        "            if module.in_proj_bias is not None:\n",
        "                nn.init.zeros_(module.in_proj_bias)\n",
        "            nn.init.normal_(module.out_proj.weight, mean=0.0, std=1e-3)\n",
        "            if module.out_proj.bias is not None:\n",
        "                nn.init.zeros_(module.out_proj.bias)\n",
        "    for name, param in block.named_parameters():\n",
        "        if \"scale\" in name:\n",
        "            param.data.fill_(1e-3)\n",
        "\n",
        "\n",
        "def init_liquid_block(block: nn.Module) -> None:\n",
        "    for module in block.modules():\n",
        "        if isinstance(module, nn.LayerNorm):\n",
        "            module.weight.data.fill_(1.0)\n",
        "            module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n",
        "            module.weight.data.mul_(1e-3)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.MultiheadAttention):\n",
        "            nn.init.kaiming_uniform_(module.in_proj_weight, a=math.sqrt(5))\n",
        "            module.in_proj_weight.data.mul_(1e-3)\n",
        "            if module.in_proj_bias is not None:\n",
        "                nn.init.zeros_(module.in_proj_bias)\n",
        "            nn.init.kaiming_uniform_(module.out_proj.weight, a=math.sqrt(5))\n",
        "            module.out_proj.weight.data.mul_(1e-3)\n",
        "            if module.out_proj.bias is not None:\n",
        "                nn.init.zeros_(module.out_proj.bias)\n",
        "    for name, param in block.named_parameters():\n",
        "        if \"scale\" in name or \"gate\" in name:\n",
        "            param.data.fill_(1e-3)\n",
        "\n",
        "\n",
        "def init_new_layers(model: Stage1Model, base_layers: int, add_classic: int, add_liquid: int) -> None:\n",
        "    total_blocks = len(model.blocks)\n",
        "    for idx in range(base_layers, min(base_layers + add_classic, total_blocks)):\n",
        "        if isinstance(model.blocks[idx], ClassicBlock):\n",
        "            init_classic_block(model.blocks[idx])\n",
        "    start_liquid = base_layers + add_classic\n",
        "    for idx in range(start_liquid, min(start_liquid + add_liquid, total_blocks)):\n",
        "        if isinstance(model.blocks[idx], LiquidBlock):\n",
        "            init_liquid_block(model.blocks[idx])\n",
        "\n",
        "\n",
        "def gather_layer_types(model: Stage1Model) -> List[str]:\n",
        "    layout: List[str] = []\n",
        "    for block in model.blocks:\n",
        "        if isinstance(block, ClassicBlock):\n",
        "            layout.append(\"classic\")\n",
        "        elif isinstance(block, LiquidBlock):\n",
        "            layout.append(\"liquid\")\n",
        "        else:\n",
        "            layout.append(type(block).__name__)\n",
        "    return layout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: artifact writers, reporting, and optional dry-run\n",
        "\n",
        "def maybe_save_safetensors(\n",
        "    state_dict: Dict[str, torch.Tensor], path: str, metadata: Dict[str, Any]\n",
        ") -> Optional[str]:\n",
        "    try:\n",
        "        from safetensors.torch import save_file  # type: ignore\n",
        "\n",
        "        meta = {k: str(v) for k, v in metadata.items() if isinstance(v, (str, int, float))}\n",
        "        save_file(state_dict, path, metadata=meta)\n",
        "        return path\n",
        "    except Exception as exc:  # pragma: no cover - optional dependency\n",
        "        print(f\"safetensors save unavailable ({exc}); falling back to torch.save\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_checkpoint(\n",
        "    model: Stage1Model,\n",
        "    output_dir: str,\n",
        "    metadata: Dict[str, Any],\n",
        ") -> str:\n",
        "    timestamp = datetime.datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    base = f\"stage1_surgery_{timestamp}\"\n",
        "    safepath = os.path.join(output_dir, base + \".safetensors\")\n",
        "    state = {k: v.detach().to(\"cpu\") for k, v in model.state_dict().items()}\n",
        "    saved = maybe_save_safetensors(state, safepath, metadata)\n",
        "    if saved:\n",
        "        return saved\n",
        "    pypath = os.path.join(output_dir, base + \".pt\")\n",
        "    torch.save({\"state_dict\": state, \"metadata\": metadata}, pypath)\n",
        "    return pypath\n",
        "\n",
        "\n",
        "def write_json(path: str, payload: Dict[str, Any]) -> None:\n",
        "    with open(path, \"w\") as handle:\n",
        "        json.dump(payload, handle, indent=2, sort_keys=True)\n",
        "\n",
        "\n",
        "def write_freeze_mask(\n",
        "    output_dir: str,\n",
        "    base_layers: int,\n",
        "    add_classic: int,\n",
        "    total_layers: int,\n",
        ") -> str:\n",
        "    mask = {}\n",
        "    for idx in range(total_layers):\n",
        "        mask[f\"blocks.{idx}\"] = bool(base_layers <= idx < base_layers + add_classic)\n",
        "    path = os.path.join(output_dir, \"freeze_mask.json\")\n",
        "    write_json(path, mask)\n",
        "    return path\n",
        "\n",
        "\n",
        "def write_config(output_dir: str, config: ModelConfig, extras: Dict[str, Any]) -> str:\n",
        "    cfg = asdict(config)\n",
        "    cfg.update(extras)\n",
        "    path = os.path.join(output_dir, \"config_stage1.json\")\n",
        "    write_json(path, cfg)\n",
        "    return path\n",
        "\n",
        "\n",
        "def write_report(\n",
        "    output_dir: str,\n",
        "    stats: Dict[str, Any],\n",
        "    target_d_model: int,\n",
        "    copied: List[str],\n",
        "    widened: List[str],\n",
        "    skipped: List[str],\n",
        "    missing: List[str],\n",
        "    unexpected: List[str],\n",
        "    artifacts: Dict[str, str],\n",
        "    layer_types: List[str],\n",
        "    dry_run: Optional[Dict[str, Any]] = None,\n",
        ") -> str:\n",
        "    report_path = os.path.join(output_dir, \"surgery_report.md\")\n",
        "    lines = [\n",
        "        \"# Stage-1 Surgery Report\",\n",
        "        \"\",\n",
        "        f\"Source checkpoint: `{CHECKPOINT_PATH}`\",\n",
        "        f\"Output checkpoint: `{artifacts['weights']}`\",\n",
        "        \"\",\n",
        "        \"## Inferred Stage-0\",\n",
        "        f\"- n_layers: {stats['n_layers']}\",\n",
        "        f\"- d_model: {stats['d_model']}\",\n",
        "        f\"- vocab_size: {stats['vocab_size']}\",\n",
        "        f\"- n_heads: {stats.get('n_heads')}\",\n",
        "        \"## Stage-1 Target\",\n",
        "        f\"- widened d_model: {stats['d_model']} -> {target_d_model}\",\n",
        "        f\"- total layers: {len(layer_types)} (layout: {layer_types})\",\n",
        "        \"\",\n",
        "        \"## Tensor Copy Stats\",\n",
        "        f\"- copied (same shape): {len(copied)}\",\n",
        "        f\"- widened (Net2Net): {len(widened)}\",\n",
        "        f\"- kept/init destination: {len(skipped)}\",\n",
        "        \"\",\n",
        "        \"## Load Diagnostics\",\n",
        "        f\"- missing keys (strict=False): {len(missing)}\",\n",
        "        f\"- unexpected keys: {len(unexpected)}\",\n",
        "    ]\n",
        "    if dry_run is not None:\n",
        "        lines.extend(\n",
        "            [\n",
        "                \"\",\n",
        "                \"## Dry-Run Cosine Similarity\",\n",
        "                f\"- ran: {dry_run['status']}\",\n",
        "            ]\n",
        "        )\n",
        "        if dry_run.get(\"status\") == \"ok\":\n",
        "            lines.append(f\"- cosine similarity: {dry_run['cosine']:.6f}\")\n",
        "            lines.append(f\"- widen missing keys: {dry_run['missing']}\")\n",
        "            lines.append(f\"- widen unexpected keys: {dry_run['unexpected']}\")\n",
        "        elif dry_run.get(\"message\"):\n",
        "            lines.append(f\"- note: {dry_run['message']}\")\n",
        "    lines.extend(\n",
        "        [\n",
        "            \"\",\n",
        "            \"Artifacts:\",\n",
        "            f\"- weights: {artifacts['weights']}\",\n",
        "            f\"- config: {artifacts['config']}\",\n",
        "            f\"- freeze mask: {artifacts['freeze_mask']}\",\n",
        "        ]\n",
        "    )\n",
        "    with open(report_path, \"w\") as handle:\n",
        "        handle.write(\"\n",
        "\".join(lines) + \"\n",
        "\")\n",
        "    return report_path\n",
        "\n",
        "\n",
        "def attempt_dry_run(\n",
        "    src_state: Dict[str, torch.Tensor],\n",
        "    stats: Dict[str, Any],\n",
        "    target_d_model: int,\n",
        "    widen_pct: float,\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    if not ENABLE_DRY_RUN:\n",
        "        return None\n",
        "    if StudentLM is None:\n",
        "        return {\"status\": \"skipped\", \"message\": \"StudentLM import unavailable\"}\n",
        "    if stats.get(\"n_heads\") is None:\n",
        "        return {\"status\": \"skipped\", \"message\": \"n_heads unknown; set N_HEADS_OVERRIDE\"}\n",
        "    try:\n",
        "        student = StudentLM(\n",
        "            stats[\"vocab_size\"],\n",
        "            d_model=stats[\"d_model\"],\n",
        "            n_layers=stats[\"n_layers\"],\n",
        "            n_heads=stats[\"n_heads\"],\n",
        "        ).to(SURGERY_DTYPE)\n",
        "        load_res = student.load_state_dict(src_state, strict=False)\n",
        "        if load_res:\n",
        "            print(f\"StudentLM load differences: {load_res}\")\n",
        "    except Exception as exc:\n",
        "        return {\"status\": \"skipped\", \"message\": f\"failed to instantiate StudentLM: {exc}\"}\n",
        "\n",
        "    try:\n",
        "        cfg = ModelConfig(\n",
        "            vocab_size=stats[\"vocab_size\"],\n",
        "            d_model=stats[\"d_model\"],\n",
        "            n_heads=stats[\"n_heads\"],\n",
        "            n_layers=stats[\"n_layers\"],\n",
        "            max_seq_len=stats[\"max_seq_len\"],\n",
        "            widen_pct=widen_pct,\n",
        "            add_classic=0,\n",
        "            add_liquid=0,\n",
        "        )\n",
        "        widen_model = Stage1Model(cfg).to(SURGERY_DTYPE)\n",
        "        dest_state = widen_model.state_dict()\n",
        "        transplanted, _, _, _, _ = transplant_weights(\n",
        "            src_state, dest_state, stats[\"n_layers\"], stats[\"d_model\"], target_d_model, WIDTH_TOLERANCE\n",
        "        )\n",
        "        missing, unexpected = widen_model.load_state_dict(transplanted, strict=False)\n",
        "        widen_model.eval()\n",
        "    except Exception as exc:\n",
        "        return {\"status\": \"skipped\", \"message\": f\"widen-only model load failed: {exc}\"}\n",
        "\n",
        "    student.eval()\n",
        "    with torch.no_grad():\n",
        "        tokens = torch.randint(0, stats[\"vocab_size\"], (DRY_RUN_BATCH, DRY_RUN_SEQ))\n",
        "        student_out = student(tokens)\n",
        "        logits0 = student_out[0] if isinstance(student_out, tuple) else student_out\n",
        "        widen_out = widen_model(tokens.to(logits0.device))\n",
        "        logits1 = widen_out[0] if isinstance(widen_out, tuple) else widen_out\n",
        "        cos = F.cosine_similarity(\n",
        "            logits0.reshape(logits0.size(0), -1), logits1.reshape(logits1.size(0), -1)\n",
        "        )\n",
        "        cosine = float(cos.mean().item())\n",
        "    status = \"ok\" if cosine >= DRY_RUN_COS_TGT else \"warn\"\n",
        "    return {\n",
        "        \"status\": status,\n",
        "        \"cosine\": cosine,\n",
        "        \"missing\": len(missing),\n",
        "        \"unexpected\": len(unexpected),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: run the Stage-1 surgery end-to-end\n",
        "\n",
        "def run_stage1_surgery() -> Dict[str, Any]:\n",
        "    src_state, metadata = load_checkpoint(CHECKPOINT_PATH)\n",
        "    stats = infer_model_stats(src_state, metadata)\n",
        "    if stats.get(\"n_heads\") is None:\n",
        "        if N_HEADS_OVERRIDE is None:\n",
        "            raise ValueError(\n",
        "                \"Unable to infer n_heads from checkpoint metadata. Set N_HEADS_OVERRIDE manually.\"\n",
        "            )\n",
        "        stats[\"n_heads\"] = N_HEADS_OVERRIDE\n",
        "    elif N_HEADS_OVERRIDE is not None:\n",
        "        stats[\"n_heads\"] = int(N_HEADS_OVERRIDE)\n",
        "\n",
        "    target_d_model = compute_target_width(\n",
        "        stats[\"d_model\"], WIDTH_SCALE, ROUND_MULT, stats.get(\"n_heads\")\n",
        "    )\n",
        "    effective_scale = target_d_model / float(stats[\"d_model\"])\n",
        "    widen_pct = (effective_scale - 1.0) * 100.0\n",
        "\n",
        "    print(\"--- Stage-0 inference ---\")\n",
        "    for key in [\"n_layers\", \"d_model\", \"vocab_size\", \"max_seq_len\", \"n_heads\"]:\n",
        "        print(f\"{key:>12}: {stats.get(key)}\")\n",
        "    print(f\"embed key: {stats['embed_key']}\")\n",
        "\n",
        "    dry_run = attempt_dry_run(src_state, stats, target_d_model, widen_pct)\n",
        "    if dry_run is not None:\n",
        "        print(f\"Dry-run status: {dry_run.get('status')} (cosine={dry_run.get('cosine')})\")\n",
        "\n",
        "    config = ModelConfig(\n",
        "        vocab_size=stats[\"vocab_size\"],\n",
        "        d_model=stats[\"d_model\"],\n",
        "        n_heads=stats[\"n_heads\"],\n",
        "        n_layers=stats[\"n_layers\"],\n",
        "        max_seq_len=stats[\"max_seq_len\"],\n",
        "        widen_pct=widen_pct,\n",
        "        add_classic=ADD_CLASSIC_LAY,\n",
        "        add_liquid=ADD_LIQUID_LAY,\n",
        "    )\n",
        "    model = Stage1Model(config).to(SURGERY_DTYPE)\n",
        "    init_new_layers(model, stats[\"n_layers\"], ADD_CLASSIC_LAY, ADD_LIQUID_LAY)\n",
        "\n",
        "    dest_state = model.state_dict()\n",
        "    transplanted, copied, widened, skipped, mapping = transplant_weights(\n",
        "        src_state,\n",
        "        dest_state,\n",
        "        stats[\"n_layers\"],\n",
        "        stats[\"d_model\"],\n",
        "        target_d_model,\n",
        "        WIDTH_TOLERANCE,\n",
        "    )\n",
        "    missing, unexpected = model.load_state_dict(transplanted, strict=False)\n",
        "\n",
        "    layer_types = gather_layer_types(model)\n",
        "    new_param_keys = {k for k in dest_state if is_new_layer_key(k, stats[\"n_layers\"])}\n",
        "    skipped_existing = [k for k in skipped if k not in new_param_keys]\n",
        "\n",
        "    print(\"\n",
        "--- Stage-1 summary ---\")\n",
        "    print(f\"target d_model: {target_d_model} (scale={effective_scale:.4f}, widen_pct={widen_pct:.2f})\")\n",
        "    print(f\"blocks: base={stats['n_layers']} + classic={ADD_CLASSIC_LAY} + liquid={ADD_LIQUID_LAY}\")\n",
        "    print(f\"copied tensors: {len(copied)}\")\n",
        "    print(f\"widened tensors: {len(widened)}\")\n",
        "    print(f\"new/init tensors: {len(skipped)} (new layers={len(new_param_keys)}, unmatched existing={len(skipped_existing)})\")\n",
        "    print(f\"load_state_dict missing={len(missing)} unexpected={len(unexpected)}\")\n",
        "\n",
        "    metadata_out = {\n",
        "        \"source_path\": CHECKPOINT_PATH,\n",
        "        \"d_model_before\": stats[\"d_model\"],\n",
        "        \"d_model_after\": target_d_model,\n",
        "        \"effective_width_scale\": effective_scale,\n",
        "        \"added_classic\": ADD_CLASSIC_LAY,\n",
        "        \"added_liquid\": ADD_LIQUID_LAY,\n",
        "        \"timestamp\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
        "    }\n",
        "\n",
        "    weights_path = save_checkpoint(model, OUTPUT_DIR, metadata_out)\n",
        "    config_path = write_config(\n",
        "        OUTPUT_DIR,\n",
        "        config,\n",
        "        {\n",
        "            \"layer_types\": layer_types,\n",
        "            \"target_d_model\": target_d_model,\n",
        "            \"effective_width_scale\": effective_scale,\n",
        "        },\n",
        "    )\n",
        "    freeze_path = write_freeze_mask(\n",
        "        OUTPUT_DIR, stats[\"n_layers\"], ADD_CLASSIC_LAY, len(layer_types)\n",
        "    )\n",
        "    report_path = write_report(\n",
        "        OUTPUT_DIR,\n",
        "        stats,\n",
        "        target_d_model,\n",
        "        copied,\n",
        "        widened,\n",
        "        skipped,\n",
        "        list(missing),\n",
        "        list(unexpected),\n",
        "        {\n",
        "            \"weights\": weights_path,\n",
        "            \"config\": config_path,\n",
        "            \"freeze_mask\": freeze_path,\n",
        "        },\n",
        "        layer_types,\n",
        "        dry_run=dry_run,\n",
        "    )\n",
        "\n",
        "    print(\"\n",
        "Artifacts written:\")\n",
        "    print(f\"- weights:      {weights_path}\")\n",
        "    print(f\"- config:       {config_path}\")\n",
        "    print(f\"- freeze mask:  {freeze_path}\")\n",
        "    print(f\"- report:       {report_path}\")\n",
        "\n",
        "    return {\n",
        "        \"stats\": stats,\n",
        "        \"target_d_model\": target_d_model,\n",
        "        \"copied\": copied,\n",
        "        \"widened\": widened,\n",
        "        \"skipped\": skipped,\n",
        "        \"missing\": missing,\n",
        "        \"unexpected\": unexpected,\n",
        "        \"weights_path\": weights_path,\n",
        "        \"config_path\": config_path,\n",
        "        \"freeze_path\": freeze_path,\n",
        "        \"report_path\": report_path,\n",
        "        \"dry_run\": dry_run,\n",
        "        \"mapping\": mapping,\n",
        "    }\n",
        "\n",
        "\n",
        "# Execute when the cell runs\n",
        "results = run_stage1_surgery()\n",
        "results\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}