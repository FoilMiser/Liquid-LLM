resume_gcs_uri: null
output_gcs_uri: ./stage1_output
teacher_name: meta-llama/Meta-Llama-3.1-8B
teacher_endpoint: null
teacher_max_batch_size: 0
seq_len: 1024
block_size: 1024
train_steps: 250000
batch_size: 8
eval_batch_size: 8
throughput_tokens: 32768
use_flash_attn: true
use_grad_ckpt: true
dtype: bfloat16
device: cuda
optimizer: adamw
lr: 0.00025
weight_decay: 0.1
betas: "0.9,0.95"
warmup_steps: 3000
eval_every: 1000
save_every: 2000
log_every: 100
max_grad_norm: 1.0
gradient_accumulation_steps: 1
hf_secret_name: hf_token
hf_cache_dir: null
seed: 42
tokenizer_name: null
d_model: 768
n_heads: 12
n_layers: 15
dropout: 0.0
layer_norm_eps: 1.0e-5
kd_temperature: 2.0
kd_alpha_start: 0.7
kd_alpha_end: 0.4
kd_anneal_pct: 0.3
keep_old_logit_l2: 0.1
keep_old_logit_l2_fade_step: 30000
keep_old_logit_l2_enable: true
dataset_cfg: ./configs/stage1.jsonl
tool_use_ratio: 0.08
calculator_enabled: true
scratchpad_enabled: true
use_checkpoint_saver: true
best_metric: val_perplexity
best_metric_mode: min
num_workers: 4
