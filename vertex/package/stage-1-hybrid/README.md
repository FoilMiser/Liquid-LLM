# Liquid LLM Stage-1 Hybrid Package

This package mirrors the original Stage-1 Vertex trainer but introduces a hybrid
knowledge distillation strategy:

* **LM data (`type=lm`)** uses an **online** teacher, `meta-llama/Llama-3.2-3B`.
* **Math/code data (`type in {math_tool, code}`)** use **precomputed** logits
  generated by the `meta-llama/Llama-3.1-8B` teacher.

All QoL features from the base package—preprocess toolkit integration, FlashAttention
installation, provenance snapshots, metrics streaming, and checkpoint uploads—remain
available.

## 1. Precompute math/code logits (Stage A)

Run the helper script to materialize logits for the math and code splits before
training. Existing files are skipped unless `--overwrite=true`.

```bash
python -m stage1_hybrid.precompute_hybrid_logits \
  --mc-teacher-id=meta-llama/Llama-3.1-8B \
  --dataset-manifest=gs://liquid-llm-bucket-2/datasets/stage1/manifests/stage1.jsonl \
  --math-logits-dir=gs://liquid-llm-bucket-2/teacher/llama-3.1-8b/logits/math/ \
  --code-logits-dir=gs://liquid-llm-bucket-2/teacher/llama-3.1-8b/logits/code/ \
  --seq-len=1024 --batch-size=4 --num-workers=4
```

The script reads the manifest (with toolkit assistance), filters `math_tool` and
`code` entries, loads the 8B teacher in inference mode, and writes one `.pt` file
per sample to the configured GCS buckets.

## 2. Launch hybrid training (Stage B)

Training mirrors the base CLI but adds hybrid controls. The example below matches
the requested defaults (L4 VM, 1024 context, KD schedules unchanged):

```bash
python -m stage1_hybrid.cli \
  --hybrid=true \
  --lm_teacher_id=meta-llama/Llama-3.2-3B \
  --mc_teacher_id=meta-llama/Llama-3.1-8B \
  --math_logits_dir=gs://liquid-llm-bucket-2/teacher/llama-3.1-8b/logits/math/ \
  --code_logits_dir=gs://liquid-llm-bucket-2/teacher/llama-3.1-8b/logits/code/ \
  --teacher_mode=online \
  --dataset_manifest=gs://liquid-llm-bucket-2/datasets/stage1/manifests/stage1.jsonl \
  --resume_gcs_uri=gs://liquid-llm-bucket-2/stage1/stage1.pt \
  --output_gcs_uri=gs://liquid-llm-bucket-2/stage1/Checkpoints/vertex-runs \
  --prepare_data=auto --seq_len=1024 --block_size=1024 \
  --kd_temperature=2.0 --kd_alpha_start=0.7 --kd_alpha_end=0.4 --kd_anneal_pct=0.3 \
  --keep_old_logit_l2=0.1 --keep_old_logit_l2_fade_step=30000 \
  --precision=bfloat16 --grad-checkpoint=true \
  --lr=2.5e-4 --weight_decay=0.1 --betas=0.9,0.95 \
  --warmup_steps=3000 --max_steps=120000 \
  --metrics-interval=100 --eval-every=1000 --save-every=2000 \
  --fa_wheel_gcs=gs://liquid-llm-bucket-2/FlashAttention/flash_attn-2.8.3+cu12torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
```

### Hybrid controls

* `--hybrid` toggles the teacher router. When enabled, the CLI builds the LM
  teacher only if LM samples are present and routes math/code batches to the
  provided precomputed directories.
* `--math_logits_dir` and `--code_logits_dir` point to the per-sample logits
  generated by the precompute step.
* `--hybrid_strict=true` enforces a conservative policy: if logits are missing
  for a math/code batch, KD is skipped for that batch and a single warning is
  emitted per split.

All other training knobs (optimizer, schedules, checkpoint cadence, dry-run) work
exactly as in the original Stage-1 package.
